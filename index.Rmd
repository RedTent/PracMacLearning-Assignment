---
title: "Predicting activities with accelerometer data"
subtitle: "Practical Machine Learning - Prediction Assignment"
author: "Johan van Tent"
date: "16-1-2020"
output: 
  html_document:
    code_folding: show

---

```{r setup}
library(tidyverse)
library(caret)
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)

# The goal of your project is to predict the manner in which they did the exercise. 
# This is the "classe" variable in the training set. You may use any of the other 
# variables to predict with. 
# You should create a report describing 
# how you built your model, 
# how you used cross validation, 
# what you think the expected out of sample error is, 
# and why you made the choices you did. 
# You will also use your prediction model to predict 20 different test cases.

# https://redtent.github.io/PracMacLearning-Assignment/
```

# Summary



# Introduction

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, the goal is to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: <http://groupware.les.inf.puc-rio.br/har> (see the section on the Weight Lifting Exercise Dataset)[^note]. 

[^note]: This introduction is a copy from the assignment text.

# Creating a model

The purpose of this exercise is to create a model that can predict the type of activity which is represented by the `classe`-variable which has either the value of `A`, `B`, `C`, `D`, or `E`. Since the goal is to predict multiple classes it makes sense to use a type of model that is able to do a multi-class prediction. A 'random forest'-model is capable of this and also produces often accurate predictions. Therefore I will try to fit a 'random forest'-model and see how well it performs. My goal is to get an prediction accuracy of 80% or better[^1].

[^1]: To pass the quiz that goes with this exercise a minimum of 80% accuarcy is required.

## Preparation

First we'll have to import the data and make some preparations before we can start to build a model. 

A lot of models, including a random forests, cannot handle missing values. For now we'll remove columns with missing values. This leaves 56 variables to predict with, which still seems reasonable. If the model performs poorly we may need to revisit this step and do some imputation. I'll also remove the id-column and the timestamp because these do not have anything to do with the type of activity and are not useful for prediction future activities.

We'll also make a split into train and test data.

```{r prep}
# import data, set missing values to NA and give the first column a proper name
dataset <- read_csv("data/pml-training.csv", na = c("NA", "#DIV/0!")) %>% rename(id = X1)

# remove missing values and the id and timestamps
dataset_sel <- dataset %>% select_if(.predicate = ~!any(is.na(.x))) %>% select(-contains("timestamp"), -id)
ncol(dataset_sel)

set.seed(1020)
in_train <- caret::createDataPartition(dataset_sel$classe, p = 0.75, list = FALSE)
train <- dataset_sel[in_train,]
test <- dataset_sel[-in_train,]

```

## Training and cross-validation

The next step is to train our model and get an estimate of the accuracy of the model. The estimated accuracy is obtained by a 10-fold cross-validation. We'll start with the default settings for a random forest in `caret::train()`. If the result is not acceptable these settings may need to be adapted.


```{r training, cache=TRUE}

# 10-fold Cross-validation
fit_control <- trainControl(method = "cv", number = 10)

set.seed(2010)
rf_fit <- train(classe ~ ., data = train, method = "rf", trControl = fit_control)

rf_fit


```

It can be seen that the final model has an accuracy of **`r round(rf_fit$results$Accuracy[2], digits = 4)`** using 10-fold cross-validation. This seems to be quite acceptable.

## Accuracy on the test set

Cross-validation suggests that the random forest model performs very well. The next step is to verify this performance on the test set.

```{r}
test_pred <- predict(rf_fit, newdata = test)
test_accuracy <- mean(test_pred == test$classe)
test_accuracy

```

The accuracy on the test set is `r round(test_accuracy, digits = 4)`. This is almost equal to the accuracy from the cross-validation.


# Predicting the quiz dataset

Finally I'll show the code for predicting the quiz dataset. However without showing the results to prevent cheating.

```{r quiz-dataset}
quiz_dataset <- read_csv("data/pml-testing.csv", na = c("NA", "#DIV/0!")) %>% rename(id = X1)

# remove missing values and the id and timestamps
quiz_dataset <- quiz_dataset %>% select_if(.predicate = ~!any(is.na(.x))) %>% select(-contains("timestamp"), -id)

quiz_prediction <- predict(rf_fit, newdata = quiz_dataset)

quiz_answers <- bind_cols(problem_id = quiz_dataset$problem_id, prediction = quiz_prediction)

```

